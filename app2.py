# app.py â€” Lighting Catalog RAGï¼ˆPDF â†’ OCR/æŠ½å– â†’ Cluster â†’ åµŒå…¥ â†’ æª¢ç´¢ï¼‰+ LLM æŸ¥è©¢è¦åŠƒ
# ä¾è³´ï¼š
#   pip install -U gradio pdfplumber pymupdf pillow pytesseract sentence-transformers numpy python-dotenv openai regex

import os, io, re, time, json, logging, warnings, hashlib, pickle, pathlib
from typing import List, Dict, Any, Tuple

import numpy as np
from PIL import Image, ImageOps, ImageFilter
import pdfplumber, fitz  # PyMuPDF
import pytesseract
import gradio as gr
import regex as rxx

from sentence_transformers import SentenceTransformer, CrossEncoder
from dotenv import load_dotenv, find_dotenv

# -----------------------------
# éœéŸ³ä¸€äº›ä¸æ˜¯éŒ¯èª¤çš„è¨Šæ¯
# -----------------------------
logging.getLogger("pdfminer").setLevel(logging.ERROR)
warnings.filterwarnings("ignore", module="fitz")

# -----------------------------
# åŸºæœ¬è¨­å®š
# -----------------------------
EMBED_MODEL_NAME   = "BAAI/bge-m3"                 # å¤šèªå‘é‡
RERANK_MODEL_NAME  = "BAAI/bge-reranker-base"      # äº¤å‰ç·¨ç¢¼ rerankï¼ˆå¯é—œï¼‰
USE_RERANK         = True

DEFAULT_OCR_LANG   = "chi_tra+eng"
CHUNK_MAX_CHARS    = 900
CHUNK_OVERLAP      = 120

# æŸ¥è©¢åŒç¾©è©ï¼ˆå¯æ“´å……ï¼‰
SYNONYMS = {
    "æš–ç™½":"3000K","é»ƒå…‰":"3000K","è‡ªç„¶å…‰":"4000K","ç™½å…‰":"5000K",
    "æµæ˜":"lm","äº®åº¦":"lm","åŠŸç‡":"W","ç“¦æ•¸":"W",
    "é˜²æ°´":"IP","é¡¯è‰²":"CRI","é¡¯è‰²æŒ‡æ•¸":"CRI",
    "è§’åº¦":"beam","å…‰æŸè§’":"beam",
    "è»Œé“ç‡ˆ":"track light","æŠ•å…‰ç‡ˆ":"flood light","å´ç‡ˆ":"downlight","æŠ•å°„ç‡ˆ":"flood light"
}

# ---- ç‡ˆæ¬¾é¡å‹å­—å…¸ï¼ˆå¯ä¾ä½ çš„è³‡æ–™æ“´å……ï¼‰----
LAMP_TYPES = [
    "å¸é ‚ç‡ˆ","å´ç‡ˆ","è»Œé“ç‡ˆ","æŠ•å…‰ç‡ˆ","æŠ•å°„ç‡ˆ","å£ç‡ˆ","åŠç‡ˆ","æª¯ç‡ˆ","ç«‹ç‡ˆ",
    "ç‡ˆå¸¶","æ—¥å…‰ç‡ˆ","è·¯ç‡ˆ","è‰åœ°ç‡ˆ","åº­åœ’ç‡ˆ","æ´—ç‰†ç‡ˆ","å¤©äº•ç‡ˆ","æ ¼æŸµç‡ˆ","ç·šå‹ç‡ˆ"
]

# åŒç¾©è© â†’ æ­£è¦åŒ–
TYPE_ALIASES = {
    "downlight":"å´ç‡ˆ","åµŒç‡ˆ":"å´ç‡ˆ","å´å…¥ç‡ˆ":"å´ç‡ˆ",
    "ceiling light":"å¸é ‚ç‡ˆ","å¸é ‚":"å¸é ‚ç‡ˆ",
    "track light":"è»Œé“ç‡ˆ","å°è»Œç‡ˆ":"è»Œé“ç‡ˆ",
    "flood light":"æŠ•å…‰ç‡ˆ","æ³›å…‰ç‡ˆ":"æŠ•å…‰ç‡ˆ",
    "spot light":"æŠ•å°„ç‡ˆ","å°„ç‡ˆ":"æŠ•å°„ç‡ˆ","æŠ•å°„":"æŠ•å°„ç‡ˆ",
    "pendant":"åŠç‡ˆ",
    "wall light":"å£ç‡ˆ",
    "strip":"ç‡ˆå¸¶","æ¢ç‡ˆ":"ç‡ˆå¸¶","ç·šç‡ˆ":"ç·šå‹ç‡ˆ","ç·šå‹":"ç·šå‹ç‡ˆ",
    "high bay":"å¤©äº•ç‡ˆ","é«˜å¤©äº•ç‡ˆ":"å¤©äº•ç‡ˆ",
    "grid":"æ ¼æŸµç‡ˆ","æ ¼æŸµ":"æ ¼æŸµç‡ˆ",
    "garden":"åº­åœ’ç‡ˆ","è‰åœ°ç‡ˆ":"è‰åœ°ç‡ˆ",
}

def _normalize_types(types: list[str]) -> list[str]:
    out = []
    for t in types or []:
        if not isinstance(t, str): continue
        t0 = t.strip().lower()
        if not t0: continue
        if t0 in TYPE_ALIASES:
            std = TYPE_ALIASES[t0]
        else:
            std = next((lt for lt in LAMP_TYPES if lt.lower() == t0), None)
            if std is None:
                std = next((lt for lt in LAMP_TYPES if t0 in lt.lower()), None)
        if std and std not in out:
            out.append(std)
    return out

# -----------------------------
# OpenAI åˆå§‹åŒ–ï¼ˆæ”¯æ´ .env + UI è‡¨æ™‚è¨­å®šï¼›æ–°/èˆŠ SDKï¼‰
# -----------------------------
DOTENV_PATH = find_dotenv(usecwd=True); load_dotenv(DOTENV_PATH or "", override=True)
OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY") or "").strip()
OPENAI = None
OPENAI_STATUS = []

def _init_openai():
    global OPENAI, OPENAI_STATUS
    OPENAI_STATUS.clear()
    if not OPENAI_API_KEY:
        OPENAI = None; OPENAI_STATUS.append("no_key"); return
    try:
        from openai import OpenAI
        OPENAI = OpenAI(api_key=OPENAI_API_KEY)
        OPENAI_STATUS.append("new_sdk_ok"); return
    except Exception as e:
        OPENAI_STATUS.append(f"new_sdk_fail:{type(e).__name__}")
    try:
        import openai
        openai.api_key = OPENAI_API_KEY
        OPENAI = openai
        OPENAI_STATUS.append("legacy_sdk_ok"); return
    except Exception as e:
        OPENAI = None; OPENAI_STATUS.append(f"legacy_sdk_fail:{type(e).__name__}")

def _chat_once(messages, model="gpt-4o-mini", max_tokens=400, temperature=0.2):
    if OPENAI is None:
        raise RuntimeError("openai_client_not_ready")
    # new sdk
    if hasattr(OPENAI, "chat") and hasattr(OPENAI.chat, "completions"):
        resp = OPENAI.chat.completions.create(
            model=model, messages=messages, max_tokens=max_tokens, temperature=temperature
        )
        return resp.choices[0].message.content.strip()
    # legacy
    import types
    if isinstance(OPENAI, types.ModuleType) and hasattr(OPENAI, "ChatCompletion"):
        resp = OPENAI.ChatCompletion.create(
            model=model, messages=messages, max_tokens=max_tokens, temperature=temperature
        )
        return resp["choices"][0]["message"]["content"].strip()
    raise RuntimeError("unsupported_openai_client")

def set_api_key_runtime(key: str) -> str:
    global OPENAI_API_KEY
    k = (key or "").strip()
    if not k.startswith("sk-"): return "âŒ çœ‹èµ·ä¾†ä¸åƒ OpenAI Keyï¼ˆéœ€ sk- é–‹é ­ï¼‰ã€‚"
    os.environ["OPENAI_API_KEY"] = k
    OPENAI_API_KEY = k
    _init_openai()
    return f"âœ… å·²å¥—ç”¨ã€‚ç‹€æ…‹ï¼š{', '.join(OPENAI_STATUS)}"

_init_openai()
print(f"[dotenv] path={DOTENV_PATH or '(none)'} key_loaded={bool(OPENAI_API_KEY)} status={','.join(OPENAI_STATUS)}")

# -----------------------------
# Tesseract è‡ªå‹•åµæ¸¬ï¼ˆWindowsï¼‰
# -----------------------------
def _maybe_set_tesseract_path():
    candidates = [
        r"C:\Program Files\Tesseract-OCR\tesseract.exe",
        r"C:\Tesseract-OCR\tesseract.exe",
    ]
    for path in candidates:
        if os.path.exists(path):
            pytesseract.pytesseract.tesseract_cmd = path
            tessdata_dir = os.path.join(os.path.dirname(path), "tessdata")
            if os.path.isdir(tessdata_dir):
                os.environ["TESSDATA_PREFIX"] = tessdata_dir
            break
_maybe_set_tesseract_path()

# -----------------------------
# åµŒå…¥èˆ‡ rerank
# -----------------------------
embed_model = SentenceTransformer(EMBED_MODEL_NAME, device="cpu")
embed_model.max_seq_length = 256  # æ§åˆ¶è¨˜æ†¶é«”
reranker   = CrossEncoder(RERANK_MODEL_NAME) if USE_RERANK else None

def embed_passages(texts: List[str]) -> np.ndarray:
    out = []
    bs = 16
    for i in range(0, len(texts), bs):
        chunk = [t[:1200] for t in texts[i:i+bs]]
        vec = embed_model.encode(chunk, convert_to_numpy=True, normalize_embeddings=True, batch_size=bs, show_progress_bar=False)
        out.append(vec)
    return np.vstack(out) if out else np.zeros((0, 1024), dtype=np.float32)

def embed_query(q: str) -> np.ndarray:
    return embed_model.encode([q], convert_to_numpy=True, normalize_embeddings=True)[0]

# -----------------------------
# OCR èˆ‡æŠ½å–
# -----------------------------
def _visible_char_count(s: str) -> int:
    return len(re.findall(r"[A-Za-z0-9\u4e00-\u9fff]", s))

def ocr_pdf_page(fitz_page, dpi: int = 400, lang: str = DEFAULT_OCR_LANG) -> str:
    mat = fitz.Matrix(dpi/72.0, dpi/72.0)
    pix = fitz_page.get_pixmap(matrix=mat, alpha=False)
    img = Image.open(io.BytesIO(pix.tobytes("png")))
    img = ImageOps.grayscale(img)
    img = ImageOps.autocontrast(img)
    img = img.filter(ImageFilter.MedianFilter(3))
    config = "--psm 6 --oem 3"
    text = pytesseract.image_to_string(img, lang=lang, config=config)
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{2,}", "\n", text).strip()
    return text

def extract_pdf_chunks(pdf_path: str, catalog_name: str,
                       use_ocr_fallback: bool = True,
                       ocr_lang: str = DEFAULT_OCR_LANG,
                       force_ocr: bool = False,
                       max_chars: int = CHUNK_MAX_CHARS,
                       overlap: int = CHUNK_OVERLAP) -> List[Dict[str, Any]]:
    chunks = []
    try:
        with pdfplumber.open(pdf_path) as pdf, fitz.open(pdf_path) as doc:
            for i, page in enumerate(pdf.pages, start=1):
                text = ""
                if not force_ocr:
                    try:
                        text = page.extract_text() or ""
                        text = re.sub(r"[ \t]+", " ", text)
                        text = re.sub(r"\n{2,}", "\n", text).strip()
                    except Exception:
                        text = ""
                need_ocr = force_ocr
                if not force_ocr and _visible_char_count(text) < 20 and use_ocr_fallback:
                    need_ocr = True
                if need_ocr:
                    try:
                        text = ocr_pdf_page(doc[i-1], dpi=400, lang=ocr_lang)
                    except Exception:
                        pass
                if not text:  # ä»ç©º
                    continue
                # åˆ‡å¡Š
                start = 0
                while start < len(text):
                    end = min(start + max_chars, len(text))
                    piece = text[start:end]
                    chunks.append({"catalog": catalog_name, "page": i, "text": piece})
                    if end == len(text): break
                    start = max(0, end - overlap)
    except Exception:
        return []
    return chunks

# -----------------------------
# PKL å¿«å–ï¼ˆæœ€å°ï¼‰
# -----------------------------
PKL_CACHE_PATH = ".rag_cache/index.pkl"

def _manifest_for_folder(folder: str) -> dict:
    base = pathlib.Path(folder)
    items = []
    for p in sorted(base.rglob("*.pdf")):
        st = p.stat()
        items.append({"p": str(p.relative_to(base)), "s": st.st_size, "m": int(st.st_mtime)})
    blob = json.dumps(items, ensure_ascii=False, sort_keys=True).encode("utf-8")
    return {"sha256": hashlib.sha256(blob).hexdigest(), "items": items}

def save_cache_pkl(index, folder: str) -> str:
    os.makedirs(os.path.dirname(PKL_CACHE_PATH), exist_ok=True)
    manifest = _manifest_for_folder(folder)
    payload = {
        "manifest": manifest,
        "embed_model": EMBED_MODEL_NAME,
        "rerank_model": RERANK_MODEL_NAME,
        "embs": index.embs,
        "cluster_vecs": index.cluster_vecs,
        "docs": index.docs,
        "built": index.built,
        "cluster_docs": index.cluster_docs,
        "cluster_built": index.cluster_built,
        "cluster_map": index.cluster_map,
        "doc2cluster": index.doc2cluster,
    }
    with open(PKL_CACHE_PATH, "wb") as f:
        pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)
    return f"ğŸ’¾ å·²å„²å­˜ PKL å¿«å–ï¼š{PKL_CACHE_PATH}"

def load_cache_pkl(index, folder: str) -> str:
    if not os.path.exists(PKL_CACHE_PATH):
        return "â„¹ï¸ æ‰¾ä¸åˆ° PKL å¿«å–ã€‚"
    try:
        with open(PKL_CACHE_PATH, "rb") as f:
            data = pickle.load(f)
    except Exception as e:
        return f"âš ï¸ è¼‰å…¥ PKL å¤±æ•—ï¼š{type(e).__name__}: {e}"

    cur = _manifest_for_folder(folder)
    old = data.get("manifest", {})
    if not old or old.get("sha256") != cur.get("sha256"):
        return "â„¹ï¸ catalogs å…§å®¹å·²è®Šæ›´ï¼Œç•¥éèˆŠ PKL å¿«å–ã€‚"

    index.docs = data.get("docs", [])
    index.built = data.get("built", False)
    index.embs = data.get("embs", None)

    index.cluster_docs = data.get("cluster_docs", [])
    index.cluster_built = data.get("cluster_built", False)
    index.cluster_vecs = data.get("cluster_vecs", None)

    index.cluster_map = data.get("cluster_map", {})
    index.doc2cluster = data.get("doc2cluster", {})

    return "âœ… å·²è¼‰å…¥ PKL å¿«å–ï¼ˆè³‡æ–™æœªè®Šå‹•ï¼‰"

# -----------------------------
# In-memory ç´¢å¼•ï¼ˆåŸåŠŸèƒ½ + æ“´å…… Clusterï¼‰
# -----------------------------
class InMemoryIndex:
    def __init__(self):
        # chunk-level
        self.docs: List[Dict[str, Any]] = []
        self.embs: np.ndarray | None = None
        self.built = False
        # cluster-level
        self.cluster_docs: List[Dict[str, Any]] = []
        self.cluster_vecs: np.ndarray | None = None
        self.cluster_built = False
        self.cluster_map = {}   # cluster_id -> {"members":[doc_idx...], "signature": str, "spec": {...}, "product_name": str}
        self.doc2cluster = {}   # doc_idx -> cluster_id

    def reset(self):
        self.docs, self.embs, self.built = [], None, False
        self.cluster_docs, self.cluster_vecs, self.cluster_built = [], None, False
        self.cluster_map.clear()
        self.doc2cluster.clear()

    def add_docs(self, docs: List[Dict[str, Any]]):
        self.docs.extend(docs)

    def build(self):
        if not self.docs:
            self.embs, self.built = None, False
            return "æ²’æœ‰å¯å»ºç«‹ç´¢å¼•çš„æ–‡ä»¶"
        texts = [d["text"] for d in self.docs]
        self.embs = embed_passages(texts)
        self.built = True
        dim = self.embs.shape[1] if hasattr(self.embs,"shape") else "?"
        return f"ç´¢å¼•å»ºç«‹å®Œæˆï¼Œå…± {len(self.docs)} ç‰‡æ®µï¼ˆç¶­åº¦ {dim}ï¼‰"

    # chunk-level searchï¼ˆä¿ç•™ï¼‰
    def search(self, query: str, top_k: int = 8) -> List[Tuple[int, float]]:
        if not self.built or self.embs is None:
            return []
        q = query
        for k,v in SYNONYMS.items(): q = q.replace(k, v)
        q_emb = embed_query(q)
        sims = (self.embs @ q_emb)
        k = min(max(top_k * 5, top_k), len(sims))
        cand_idx = np.argpartition(-sims, range(k))[:k]
        pairs = [(int(i), float(sims[i])) for i in cand_idx]
        pairs.sort(key=lambda x: x[1], reverse=True)
        pairs = pairs[:max(top_k, 1)]

        if USE_RERANK and reranker is not None and len(pairs) > 0:
            q_dup = [q] * len(pairs)
            cand_texts = [self.docs[i]["text"] for i, _ in pairs]
            scores = reranker.predict(list(zip(q_dup, cand_texts)))
            reranked = list(zip([p[0] for p in pairs], [float(s) for s in scores]))
            reranked.sort(key=lambda x: x[1], reverse=True)
            pairs = reranked[:top_k]
        return pairs

    # --------- Cluster æ§‹å»ºèˆ‡å‘é‡ ---------
    @staticmethod
    def _extract_model_no(text: str) -> str | None:
        m = re.search(r"[A-Z]{2,}[A-Z0-9][A-Z0-9\-_]{1,}", text)
        return m.group(0) if m else None

    @staticmethod
    def _parse_specs(text: str) -> Dict[str, Any]:
        spec = {"watt": None, "k": None, "cri": None, "ip": None, "beam": None, "features": [], "type": None}
        m = re.findall(r"\b(\d{1,3})\s?W\b", text, flags=re.I)
        if m: spec["watt"] = int(m[0])
        m = re.findall(r"\b(\d{3,4})\s?K\b", text, flags=re.I)
        if m: spec["k"] = int(m[0])
        m = re.findall(r"CRI\s*(?:â‰¥|â‰§)?\s*(\d{2})", text, flags=re.I)
        if m: spec["cri"] = int(m[0])
        m = re.findall(r"\bIP\s?(\d{2})\b", text, flags=re.I)
        if m: spec["ip"] = int(m[0])
        m = re.findall(r"(\d{1,3})\s?(?:Â°|åº¦)", text, flags=re.I)
        if m: spec["beam"] = int(m[0])
        feats = []
        for kw in ["é˜²æ°´","é˜²æ½®","é˜²æ¿•","é˜²çœ©","æ„Ÿæ‡‰","é«˜CRI","è€å€™","ç„¡é »é–ƒ"]:
            if kw in text:
                feats.append(kw)
        spec["features"] = list(dict.fromkeys(feats))
        # ç²—ç•¥çŒœæ¸¬ç‡ˆæ¬¾é¡å‹
        low = text.lower()
        for k, v in TYPE_ALIASES.items():
            if k in low:
                spec["type"] = v; break
        for t in LAMP_TYPES:
            if t in text:
                spec["type"] = t; break
        return spec

    @staticmethod
    def _spec_signature(spec: Dict[str, Any]) -> str:
        parts = []
        if spec.get("type"): parts.append(spec["type"])
        if spec.get("watt") is not None: parts.append(f"{spec['watt']}W")
        if spec.get("k")    is not None: parts.append(f"{spec['k']}K")
        if spec.get("cri")  is not None: parts.append(f"CRIâ‰¥{spec['cri']}")
        if spec.get("ip")   is not None: parts.append(f"IP{spec['ip']}")
        if spec.get("beam") is not None: parts.append(f"{spec['beam']}Â°")
        if spec.get("features"): parts += spec["features"][:2]
        return " ".join(parts) if parts else ""

    def build_clusters(self):
        if not self.docs:
            self.cluster_docs, self.cluster_vecs, self.cluster_built = [], None, False
            self.cluster_map.clear(); self.doc2cluster.clear()
            return "æ²’æœ‰å¯å»ºç«‹å¢é›†çš„æ–‡ä»¶"

        buckets: Dict[str, List[int]] = {}
        for i, d in enumerate(self.docs):
            t = d.get("text","")
            catalog = d.get("catalog","")
            model = self._extract_model_no(t)
            if model:
                cid = f"{catalog}:{model}"
            else:
                cid = f"{catalog}:page{d.get('page',0)}"
            buckets.setdefault(cid, []).append(i)

        cluster_docs = []
        cluster_map = {}
        doc2cluster = {}
        for cid, idxs in buckets.items():
            texts = [self.docs[i]["text"] for i in idxs]
            catalog = self.docs[idxs[0]].get("catalog","")
            pages = sorted(list(dict.fromkeys([self.docs[i].get("page",0) for i in idxs])))
            merged_text = "\n".join(texts)
            model_no = cid.split(":")[1] if ":" in cid else None
            spec = self._parse_specs(merged_text)
            signature = f"{model_no or 'Unknown'} {self._spec_signature(spec)}".strip()
            product_name = model_no or (spec.get("type") or "Unknown")

            cluster_docs.append({
                "cluster_id": cid,
                "catalog": catalog,
                "model_no": model_no,
                "product_name": product_name,
                "pages": pages,
                "spec": spec,
                "text": f"{signature}\n{merged_text}"
            })
            cluster_map[cid] = {"members": idxs, "signature": signature, "spec": spec, "product_name": product_name}
            for i in idxs:
                doc2cluster[i] = cid

        self.cluster_docs = cluster_docs
        self.cluster_map = cluster_map
        self.doc2cluster = doc2cluster
        return f"å»ºç«‹ {len(self.cluster_docs)} å€‹å¢é›†ï¼ˆç”¢å“å–®ä½ï¼‰"

    def build_cluster_embeddings(self):
        if not self.cluster_docs:
            self.cluster_vecs, self.cluster_built = None, False
            return "æ²’æœ‰å¢é›†å¯åµŒå…¥"
        texts = [c["text"] for c in self.cluster_docs]
        vecs = embed_passages(texts)
        self.cluster_vecs = vecs
        self.cluster_built = True
        return f"å¢é›†å‘é‡å»ºç«‹å®Œæˆï¼Œå…± {len(self.cluster_docs)} æ¬¾ç”¢å“"

    def _constraints_from_plan(self, plan: dict) -> Dict[str, Any]:
        min_ip = None
        tokens = [*plan.get("must_terms",[]), *plan.get("nice_to_have",[])]
        if any(t for t in tokens if ("é˜²æ°´" in t or "é˜²æ½®" in t)):
            min_ip = 44
        for t in tokens:
            m = re.search(r"IP\s?(\d{2})", t, flags=re.I)
            if m:
                ip = int(m.group(1))
                min_ip = max(min_ip or 0, ip)
        return {"min_ip": min_ip}

    def search_clusters(self, query: str, top_clusters: int = 10, per_cluster_docs: int = 3) -> Tuple[List[Tuple[str,float]], List[Tuple[int,float]]]:
        if not self.cluster_built or self.cluster_vecs is None:
            return [], []
        q = query
        for k,v in SYNONYMS.items(): q = q.replace(k, v)
        qv = embed_query(q)

        sims = (self.cluster_vecs @ qv)
        k = min(max(top_clusters * 4, top_clusters), len(sims))
        cand_idx = np.argpartition(-sims, range(k))[:k]
        cluster_pairs = [(int(i), float(sims[i])) for i in cand_idx]
        cluster_pairs.sort(key=lambda x: x[1], reverse=True)
        cluster_pairs = cluster_pairs[:top_clusters]

        doc_pairs: List[Tuple[int,float]] = []
        if self.built and self.embs is not None:
            for ci, _ in cluster_pairs:
                cid = self.cluster_docs[ci]["cluster_id"]
                idxs = self.cluster_map.get(cid,{}).get("members",[])
                sims_d = [(i, float(self.embs[i] @ qv)) for i in idxs]
                sims_d.sort(key=lambda x: x[1], reverse=True)
                doc_pairs.extend(sims_d[:per_cluster_docs])

        if USE_RERANK and reranker is not None and doc_pairs:
            q_dup = [q]*len(doc_pairs)
            texts = [self.docs[i]["text"] for i,_ in doc_pairs]
            scores = reranker.predict(list(zip(q_dup, texts)))
            doc_pairs = [(doc_pairs[i][0], float(scores[i])) for i in range(len(doc_pairs))]
            doc_pairs.sort(key=lambda x:x[1], reverse=True)

        cluster_scores: Dict[str, List[float]] = {}
        for i, s in doc_pairs:
            cid = self.doc2cluster.get(i)
            if not cid: continue
            cluster_scores.setdefault(cid, []).append(s)
        ranked = sorted(
            [(cid, max(v)) for cid, v in cluster_scores.items()],
            key=lambda x: x[1], reverse=True
        )
        return ranked, doc_pairs

    def search_clusters_with_constraints(self, query: str, plan: dict, top_clusters: int = 10) -> List[Tuple[str,float]]:
        if not self.cluster_built or self.cluster_vecs is None:
            return []
        q = query
        for k,v in SYNONYMS.items(): q = q.replace(k, v)
        qv = embed_query(q)

        cons = self._constraints_from_plan(plan)
        min_ip = cons.get("min_ip")

        candidates = []
        for ci, cdoc in enumerate(self.cluster_docs):
            ip = cdoc.get("spec",{}).get("ip")
            if (min_ip is not None) and (ip is not None) and (ip < min_ip):
                continue
            # é¡å‹éæ¿¾ï¼ˆè‹¥æœ‰ï¼‰
            plan_types = plan.get("types") or []
            ctype = cdoc.get("spec",{}).get("type")
            if plan_types and ctype and (ctype not in plan_types):
                pass
            score = float(self.cluster_vecs[ci] @ qv)
            candidates.append((ci, score))
        candidates.sort(key=lambda x: x[1], reverse=True)
        candidates = candidates[:top_clusters]

        if USE_RERANK and reranker is not None and candidates:
            q_dup = [q]*len(candidates)
            texts = [self.cluster_docs[ci]["text"] for ci,_ in candidates]
            scores = reranker.predict(list(zip(q_dup, texts)))
            candidates = [(candidates[i][0], float(scores[i])) for i in range(len(candidates))]
            candidates.sort(key=lambda x:x[1], reverse=True)

        out = []
        for ci, sc in candidates:
            out.append((self.cluster_docs[ci]["cluster_id"], float(sc)))
        return out

INDEX = InMemoryIndex()

# -----------------------------
# å¯ç”¨é¡å‹åµæ¸¬ï¼ˆä¾› LLM æç¤ºï¼‰
# -----------------------------
def infer_available_types_from_index() -> list[str]:
    if not INDEX.cluster_docs:
        return LAMP_TYPES
    text_all = " \n".join(c["text"] for c in INDEX.cluster_docs if c.get("text"))
    found = []
    low = text_all.lower()
    for t in LAMP_TYPES:
        if (t in text_all) or (t.lower() in low):
            found.append(t)
    return found or LAMP_TYPES

# -----------------------------
# å»ºç´¢å¼•ï¼ˆæƒæ catalogs/ï¼ŒOCR æŠ½å–ï¼ŒåµŒå…¥ + Cluster æ¶æ§‹ï¼‰
# -----------------------------
def list_catalog_pdfs() -> str:
    cwd = os.getcwd()
    folder = os.path.join(cwd, "catalogs")
    if not os.path.isdir(folder):
        return f"âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™å¤¾ï¼š{folder}"
    files = []
    for root, _, fs in os.walk(folder):
        for f in fs:
            if f.lower().endswith(".pdf"):
                files.append(os.path.join(root, f))
    if not files:
        return f"âš ï¸ {folder} å…§æ²’æœ‰ .pdf æª”æ¡ˆ"
    out = [f"ğŸ” ç›®å‰å·¥ä½œè·¯å¾‘ï¼š{cwd}", f"ğŸ“ƒ æ‰¾åˆ° {len(files)} æœ¬ PDFï¼š"]
    out += [" - " + p for p in files]
    return "\n".join(out)

def build_index_from_folder(ocr_lang: str, use_ocr: bool, force_ocr: bool, progress=gr.Progress(track_tqdm=True)):
    try:
        INDEX.reset()
        cwd = os.getcwd()
        folder = os.path.join(cwd, "catalogs")
        if not os.path.isdir(folder):
            yield f"âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™å¤¾ï¼š{folder}\nè«‹å»ºç«‹ catalogs/ ä¸¦æ”¾å…¥ PDFã€‚"
            return

        pdf_files = []
        for root, _, files in os.walk(folder):
            for f in files:
                if f.lower().endswith(".pdf"):
                    pdf_files.append(os.path.join(root, f))
        if not pdf_files:
            yield f"âš ï¸ åœ¨ {folder} æ²’æ‰¾åˆ°ä»»ä½• PDFã€‚"; return

        yield f"ğŸ” å·¥ä½œè·¯å¾‘ï¼š{cwd}\nğŸ“ å…± {len(pdf_files)} æœ¬ PDFï¼š\n" + "\n".join(" - "+p for p in pdf_files)

        total_chunks = 0
        for fi, pdf_path in enumerate(pdf_files, 1):
            base = os.path.basename(pdf_path)
            yield f"ğŸ“„ [{fi}/{len(pdf_files)}] é–‹å§‹è™•ç†ï¼š{base}"
            try:
                with fitz.open(pdf_path) as _doc:
                    if _doc.needs_pass:
                        yield f"âŒ æª”æ¡ˆåŠ å¯†éœ€å¯†ç¢¼ï¼š{pdf_path}"
                        continue
                    n_pages = len(_doc)

                progress(0, desc=f"OCR/æŠ½å– {base}...")
                chunks = extract_pdf_chunks(
                    pdf_path,
                    catalog_name=os.path.splitext(base)[0],
                    use_ocr_fallback=use_ocr,
                    ocr_lang=ocr_lang,
                    force_ocr=force_ocr
                )
                total_chunks += len(chunks)
                INDEX.add_docs(chunks)
                yield f"âœ… {base} åŠ å…¥ç‰‡æ®µï¼š{len(chunks)}ï¼ˆé æ•¸ï¼š{n_pages}ï¼‰"
            except Exception as e:
                yield f"âŒ è§£æå¤±æ•— {base} â†’ {type(e).__name__}: {e}"

        if not INDEX.docs:
            yield "âš ï¸ æ²’æœ‰æˆåŠŸåŠ å…¥ä»»ä½•ç‰‡æ®µï¼Œè«‹æª¢æŸ¥ OCR è¨­å®šæˆ–å‹¾ã€å¼·åˆ¶ OCRã€å†è©¦ã€‚"; return

        build_msg = INDEX.build()
        yield f"ğŸ”§ {build_msg}ï¼ˆç¸½ç‰‡æ®µï¼š{total_chunks}ï¼‰"

        cmsg = INDEX.build_clusters()
        yield f"ğŸ§© {cmsg}"
        cemsg = INDEX.build_cluster_embeddings()
        yield f"ğŸ¯ {cemsg}"

        yield f"ğŸ‰ å®Œæˆï¼šchunk ç‰‡æ®µ {len(INDEX.docs)}ã€ç”¢å“å¢é›† {len(INDEX.cluster_docs)}ã€‚"

        # è‡ªå‹•å­˜ PKL
        yield save_cache_pkl(INDEX, folder)

    except Exception as e:
        yield f"ğŸ’¥ ç™¼ç”ŸéŒ¯èª¤ï¼š{type(e).__name__}: {e}"

# -----------------------------
# LLM æŸ¥è©¢è¦åŠƒï¼ˆå«ç”¢å“å/å‹è™Ÿ/é¡å‹ï¼‰
# -----------------------------
_RULE_HINTS = [
    (rxx.compile(r"æµ´å®¤|è¡›æµ´|æ½®æ¿•|å»æ‰€"),        ["IP65","IP54","é˜²æ°´","é˜²æ½®","é˜²æ¿•"]),
    (rxx.compile(r"æˆ¶å¤–|åº­é™¢|é™½å°|é›¨"),          ["IP65","IP66","é˜²æ°´","è€å€™"]),
    (rxx.compile(r"å»šæˆ¿|æ²¹ç…™"),                  ["æ˜“æ¸…æ½”","é˜²æ²¹æ±™","IP44"]),
    (rxx.compile(r"èµ°é“|ç„é—œ|æ¨“æ¢¯"),             ["æ„Ÿæ‡‰","å¾®æ³¢æ„Ÿæ‡‰","å»£è§’","é˜²çœ©"]),
    (rxx.compile(r"å±•ç¤ºæ«ƒ|å±•æ«ƒ|é™³åˆ—"),           ["çª„è§’","15Â°","24Â°","é«˜CRI","Raâ‰¥90"]),
    (rxx.compile(r"é–±è®€|è¾¦å…¬|æ›¸æˆ¿"),             ["4000K","5000K","é˜²çœ©","ç„¡é »é–ƒ"]),
    (rxx.compile(r"æ”å½±|å•†å“æ‹æ”"),              ["é«˜CRI","Raâ‰¥90","Raâ‰¥95","é«˜æµæ˜"]),
]

def _rule_plan(user_text:str)->dict:
    q = user_text.strip()
    must, nice, types = [], [], []

    for pat,kws in _RULE_HINTS:
        if pat.search(q): nice.extend(kws)

    must += rxx.findall(r"\b\d{2,4}K\b", q, flags=rxx.I)
    must += rxx.findall(r"\b\d{1,3}\s?W\b", q, flags=rxx.I)
    must += rxx.findall(r"\bIP\s?\d{2}\b", q, flags=rxx.I)
    must += rxx.findall(r"CRI\s*(?:â‰¥|â‰§)?\s*\d{2}", q, flags=rxx.I)

    for key, std in TYPE_ALIASES.items():
        if key in q.lower():
            types.append(std)
    for t in LAMP_TYPES:
        if t in q:
            types.append(t)

    must = list(dict.fromkeys([m.replace(" ","") for m in must]))
    nice = list(dict.fromkeys(nice))
    types = _normalize_types(types)

    return {
        "product_names": [],
        "model_numbers": [],
        "types": types,
        "exclude_types": [],
        "must_terms": must,
        "nice_to_have": nice,
        "negations": []
    }

def infer_available_types_from_index() -> list[str]:
    if not INDEX.cluster_docs:
        return LAMP_TYPES
    text_all = " \n".join(c["text"] for c in INDEX.cluster_docs if c.get("text"))
    found = []
    low = text_all.lower()
    for t in LAMP_TYPES:
        if (t in text_all) or (t.lower() in low):
            found.append(t)
    return found or LAMP_TYPES

def llm_plan_query(user_text: str) -> dict:
    if OPENAI is None:
        return _rule_plan(user_text)

    if INDEX.cluster_docs:
        all_models = [c.get("model_no") for c in INDEX.cluster_docs if c.get("model_no")]
        all_names = [c.get("product_name") for c in INDEX.cluster_docs if c.get("product_name")]
        all_candidates = list(dict.fromkeys(all_models + all_names))[:80]
        sample_text = "ã€".join(all_candidates)
        catalog_hint = f"ä»¥ä¸‹æ˜¯è³‡æ–™é›†ä¸­å‡ºç¾çš„ç”¢å“æˆ–å‹è™Ÿï¼š{sample_text}ã€‚"
    else:
        catalog_hint = ""

    available_types = infer_available_types_from_index()
    types_csv = "ã€".join(available_types)

    sys = (
        "ä½ æ˜¯å°ˆæ¥­ç‡ˆå…·é¡§å•èˆ‡æª¢ç´¢åŠ©æ‰‹ã€‚è«‹é–±è®€ä½¿ç”¨è€…éœ€æ±‚ï¼Œä¸¦æ ¹æ“šæä¾›çš„å€™é¸å‹è™Ÿæ¸…å–®èˆ‡è³‡æ–™ï¼Œ"
        "è¼¸å‡º JSONï¼ŒéµåŒ…å«ï¼šproduct_namesã€model_numbersã€typesã€exclude_typesã€must_termsã€nice_to_haveã€negationsã€‚"
        "è‹¥ä½¿ç”¨è€…è¼¸å…¥åŒ…å«å…·é«”å“ç‰Œã€å‹è™Ÿï¼Œæˆ–èˆ‡ç¾æœ‰ PDF ç›¸ç¬¦ä¹‹åç¨±åŠå–®è©ï¼Œè«‹å„ªå…ˆåˆ—æ–¼ product_names / model_numbersã€‚"
        "å¦‚æœèˆ‡ç¾æœ‰pdfæœ‰ç›´æ¥æ¨è–¦é—œè¯ï¼Œè«‹å„ªå…ˆåˆ—æ–¼ product_names / model_numbersã€‚"
        "è¦æ ¼è«‹ç”¨æ¨™æº–æ¨™è¨˜ï¼ˆ3000Kã€20Wã€CRIâ‰¥90ã€IP65ã€15Â°/24Â°ã€é˜²æ°´/é˜²æ½®/é˜²çœ©/æ„Ÿæ‡‰/é«˜CRIï¼‰ã€‚"
        "åªå› JSONï¼Œä¸è¦å¤šé¤˜èªªæ˜ã€‚"
        f"å€™é¸ç‡ˆæ¬¾é¡å‹ï¼š{types_csv}ã€‚{catalog_hint}"
    )

    fewshot_user = "æˆ‘æƒ³æ‰¾ IP65 é˜²æ°´çš„æµ´å®¤å¸é ‚ç‡ˆ DL-123"
    fewshot_assistant = json.dumps({
        "product_names": ["DL-123 å¸é ‚ç‡ˆ"],
        "model_numbers": ["DL-123"],
        "types": ["å¸é ‚ç‡ˆ"],
        "exclude_types": [],
        "must_terms": ["IP65", "é˜²æ°´"],
        "nice_to_have": ["é˜²æ½®", "3000K"],
        "negations": []
    }, ensure_ascii=False)

    usr = f"""ä½¿ç”¨è€…éœ€æ±‚ï¼š{user_text}
è«‹å›å‚³ JSONï¼Œä¾‹å¦‚ï¼š
{{
 "product_names":["DL-123 å¸é ‚ç‡ˆ"],
 "model_numbers":["DL-123"],
 "types":["å¸é ‚ç‡ˆ"],
 "exclude_types":[],
 "must_terms":["IP65","é˜²æ°´"],
 "nice_to_have":["é˜²çœ©","é«˜CRI"],
 "negations":[]
}}"""

    try:
        text = _chat_once(
            [
                {"role": "system", "content": sys},
                {"role": "user", "content": fewshot_user},
                {"role": "assistant", "content": fewshot_assistant},
                {"role": "user", "content": usr},
            ],
            model="gpt-4o-mini", max_tokens=250, temperature=0.15
        )
        data = json.loads(text)
        if not isinstance(data, dict):
            raise ValueError("not dict")

        for k in ("product_names","model_numbers","types","exclude_types",
                  "must_terms","nice_to_have","negations"):
            if k not in data or not isinstance(data[k], list):
                data[k] = []

        data["types"] = [t for t in _normalize_types(data["types"]) if t in available_types][:2]
        data["exclude_types"] = [t for t in _normalize_types(data["exclude_types"]) if t in available_types][:3]
        return data
    except Exception as e:
        print("llm_plan_query fallback:", type(e).__name__, e)
        return _rule_plan(user_text)

def build_search_string(plan: dict) -> str:
    must = plan.get("must_terms", [])
    nice = plan.get("nice_to_have", [])
    types = plan.get("types", [])
    prods = plan.get("product_names", []) + plan.get("model_numbers", [])
    tokens = []
    tokens += prods * 3
    tokens += types * 2
    tokens += must * 2
    tokens += nice
    return " ".join(tokens) if tokens else ""

# -----------------------------
# æŸ¥è©¢ï¼ˆç‰‡æ®µå±¤ï¼Œä¿ç•™åŸæœ¬ï¼‰
# -----------------------------
def ask_basic(query: str, top_k: int = 6) -> str:
    if not INDEX.built:
        return "âš ï¸ è«‹å…ˆå»ºç«‹ç´¢å¼•ã€‚"
    if not query.strip():
        return "è«‹è¼¸å…¥æŸ¥è©¢æ¢ä»¶ã€‚"
    hits = INDEX.search(query.strip(), top_k=top_k)
    if not hits: return "âŒ æ‰¾ä¸åˆ°ç›¸ç¬¦å…§å®¹ã€‚"
    out=[]
    spec_re = {
        "ç“¦æ•¸":   r"\b(\d{1,3}\s?W)\b",
        "è‰²æº«":   r"\b(\d{3,4}\s?K)\b",
        "CRI":    r"(CRI\s*(?:â‰¥|â‰§)?\s*\d{2})",
        "å…‰æŸè§’": r"(\d{1,3}\s?(?:Â°|åº¦))",
        "IP":     r"\bIP\s?\d{2}\b",
        "æµæ˜":   r"\b\d{3,5}\s?lm\b",
        "ç‰¹æ€§":   r"(é˜²æ°´|é˜²æ½®|é˜²æ¿•|é˜²çœ©|æ„Ÿæ‡‰|é«˜CRI|è€å€™|ç„¡é »é–ƒ)"
    }
    for idx,_ in hits:
        d = INDEX.docs[idx]
        sheet = f"{d.get('catalog','é é¢')} p.{d.get('page','?')}"
        text  = d.get("text","").replace("\n"," ")
        bullets=[]
        for k,rg in spec_re.items():
            ms = list(re.finditer(rg, text, re.I))
            if ms:
                vals = list(dict.fromkeys([m.group(1) if m.lastindex else m.group(0) for m in ms]))
                bullets.append(f"- **{k}**ï¼š{', '.join(vals[:4])}")
        if not bullets:
            bullets.append(f"- å…§å®¹ï¼š{text[:160]}{'â€¦' if len(text)>160 else ''}")
        out.append(f"### {sheet}\n" + "\n".join(bullets))
    return "\n\n".join(out)

def ask_ai(user_text: str, top_k: int = 6) -> str:
    if not INDEX.built:
        return "âš ï¸ è«‹å…ˆå»ºç«‹ç´¢å¼•ã€‚"
    plan = llm_plan_query(user_text)
    q = build_search_string(plan) or user_text
    hits = INDEX.search(q, top_k=top_k)
    if not hits:
        return f"âŒ æ‰¾ä¸åˆ°çµæœï¼ˆæŸ¥è©¢å­—ä¸²ï¼š{q}ï¼‰"
    header = (
        f"**æŸ¥è©¢ç†è§£** â†’ products={plan.get('product_names',[])+plan.get('model_numbers',[])}, "
        f"types={plan.get('types',[])}, must={plan.get('must_terms',[])}, nice={plan.get('nice_to_have',[])}"
    )
    return header + "\n\n" + ask_basic(q, top_k)

# -----------------------------
# ç”¢å“ç‚ºä¸­å¿ƒï¼ˆClusterï¼‰æŸ¥è©¢
# -----------------------------
def _fmt_product_card(cid: str, score: float) -> str:
    meta = INDEX.cluster_map.get(cid, {})
    spec = meta.get("spec", {})
    sig  = meta.get("signature", "")
    cdoc = next((c for c in INDEX.cluster_docs if c["cluster_id"]==cid), None)
    pages = cdoc.get("pages", []) if cdoc else []
    catalog = cdoc.get("catalog","?") if cdoc else "?"
    model_no = cdoc.get("model_no","Unknown") if cdoc else "Unknown"
    product_name = cdoc.get("product_name","Unknown") if cdoc else "Unknown"

    line = []
    if spec.get("type"): line.append(spec["type"])
    if spec.get("watt") is not None: line.append(f"{spec['watt']}W")
    if spec.get("k")    is not None: line.append(f"{spec['k']}K")
    if spec.get("cri")  is not None: line.append(f"CRIâ‰¥{spec['cri']}")
    if spec.get("ip")   is not None: line.append(f"IP{spec['ip']}")
    if spec.get("beam") is not None: line.append(f"{spec['beam']}Â°")
    if spec.get("features"): line += spec["features"][:2]
    spec_line = " / ".join(line) if line else "(ç„¡è§£æåˆ°è¦æ ¼)"

    return (
        f"### {product_name} | å‹è™Ÿï¼š{model_no} ã€”{catalog}ã€•  \n"
        f"- **è¦æ ¼**ï¼š{spec_line}  \n"
        f"- **é ç¢¼**ï¼š{pages[:8]}{'â€¦' if len(pages)>8 else ''}  \n"
        f"- **ç›¸ä¼¼åº¦/åˆ†æ•¸**ï¼š{score:.4f}  \n"
        f"- **æ‘˜è¦**ï¼š{sig}\n"
    )

def ask_product(query: str, top_k: int = 6) -> str:
    if not INDEX.cluster_built:
        return "âš ï¸ éœ€å…ˆå»ºç«‹å¢é›†ç´¢å¼•ï¼ˆæŒ‰ã€æƒæä¸¦å»ºç«‹ç´¢å¼•ã€æœƒè‡ªå‹•å®Œæˆï¼‰ã€‚"
    if not query.strip():
        return "è«‹è¼¸å…¥æŸ¥è©¢æ¢ä»¶ã€‚"
    ranked, _docpairs = INDEX.search_clusters(query.strip(), top_clusters=max(top_k*2, top_k), per_cluster_docs=3)
    if not ranked:
        return "âŒ æ‰¾ä¸åˆ°ç›¸ç¬¦ç”¢å“ã€‚"
    out = []
    for cid, sc in ranked[:top_k]:
        out.append(_fmt_product_card(cid, sc))
    return "\n\n".join(out)

def ask_product_ai(user_text: str, top_k: int = 6) -> str:
    if not INDEX.cluster_built:
        return "âš ï¸ éœ€å…ˆå»ºç«‹å¢é›†ç´¢å¼•ï¼ˆæŒ‰ã€æƒæä¸¦å»ºç«‹ç´¢å¼•ã€æœƒè‡ªå‹•å®Œæˆï¼‰ã€‚"
    plan = llm_plan_query(user_text)
    q = build_search_string(plan) or user_text
    ranked = INDEX.search_clusters_with_constraints(q, plan, top_clusters=max(top_k*3, top_k))
    if not ranked:
        ranked, _ = INDEX.search_clusters(q, top_clusters=max(top_k*2, top_k), per_cluster_docs=3)
        if not ranked:
            return f"âŒ æ‰¾ä¸åˆ°çµæœï¼ˆæŸ¥è©¢å­—ä¸²ï¼š{q}ï¼‰"
    header = (
        f"**æŸ¥è©¢ç†è§£ï¼ˆç”¢å“ï¼‰** â†’ products={plan.get('product_names',[])+plan.get('model_numbers',[])}, "
        f"types={plan.get('types',[])}, must={plan.get('must_terms',[])}, nice={plan.get('nice_to_have',[])}"
    )
    cards = "\n\n".join(_fmt_product_card(cid, sc) for cid, sc in ranked[:top_k])
    return header + "\n\n" + cards

# -----------------------------
# UI + å¿«æ·éµï¼ˆåªé‡å° PKL å¿«å–ï¼‰
# -----------------------------
def _ui_load_cache_pkl():
    folder = os.path.join(os.getcwd(), "catalogs")
    return load_cache_pkl(INDEX, folder)

def _ui_save_cache_pkl():
    folder = os.path.join(os.getcwd(), "catalogs")
    return save_cache_pkl(INDEX, folder)

HOTKEY_JS = """
<script>
(function(){
  const clickById = (id) => { const el = document.getElementById(id); if(el) el.click(); }
  document.addEventListener('keydown', function(e){
    if(!e.ctrlKey) return;
    const k = e.key.toLowerCase();
    if(k==='l'){ e.preventDefault(); clickById('btn_load_pkl'); } // Ctrl+L è¼‰å…¥ PKL
    if(k==='s'){ e.preventDefault(); clickById('btn_save_pkl'); } // Ctrl+S å„²å­˜ PKL
  }, true);
})();
</script>
"""

with gr.Blocks(title="Lighting Catalog RAG â€“ PDF + LLM æŸ¥è©¢è¦åŠƒï¼ˆå«ç”¢å“å¢é›†ï¼‰") as demo:
    gr.HTML(HOTKEY_JS)
    gr.Markdown(
        "# ğŸ’¡ Lighting Catalog RAG â€“ PDF + LLM æŸ¥è©¢è¦åŠƒï¼ˆå«ç”¢å“å¢é›†ï¼‰\n"
        "- å°‡ PDF æ”¾åœ¨å°ˆæ¡ˆæ ¹ç›®çš„ **catalogs/** â†’ æŒ‰ã€Œæƒæä¸¦å»ºç«‹ç´¢å¼•ã€ã€‚\n"
        "- å¯ç”¨ã€é—œéµå­—ç›´æ¥æœã€æˆ–ã€AI ç†è§£å¾Œæœå°‹ã€ï¼Œä»¥åŠã€**ä»¥ç”¢å“ç‚ºä¸­å¿ƒæœå°‹**ã€ã€‚\n"
        "- **å¿«æ·éµ**ï¼šCtrl+L è¼‰å…¥ PKLã€Ctrl+S å„²å­˜ PKLã€‚"
    )

    with gr.Row():
        ocr_lang = gr.Dropdown(choices=["chi_tra+eng","chi_sim+eng","eng"], value=DEFAULT_OCR_LANG, label="OCR èªè¨€ï¼ˆTesseractï¼‰")
        use_ocr = gr.Checkbox(value=True, label="éœ€è¦æ™‚è‡ªå‹•ä½¿ç”¨ OCRï¼ˆåœ–ç‰‡é é¢ï¼‰")
        force_ocr = gr.Checkbox(value=False, label="å¼·åˆ¶ OCRï¼ˆæ¯é éƒ½ OCRï¼‰")

    with gr.Row():
        build_btn = gr.Button("â‘  æƒæ catalogs ä¸¦å»ºç«‹ç´¢å¼•ï¼ˆå«ç”¢å“å¢é›†ï¼‰", elem_id="btn_build", scale=2)
        peek_btn = gr.Button("ğŸ” æª¢è¦– catalogs å…§å®¹", scale=1)
        key_in = gr.Textbox(label="ï¼ˆå¯é¸ï¼‰è‡¨æ™‚è¨­å®š OpenAI API Keyï¼šsk-xxxx", type="password")
        key_btn = gr.Button("å¥—ç”¨Key", scale=1)
        load_pkl_btn = gr.Button("è¼‰å…¥ PKL å¿«å–", elem_id="btn_load_pkl", scale=1)
        save_pkl_btn = gr.Button("å„²å­˜ PKL å¿«å–", elem_id="btn_save_pkl", scale=1)

    build_status = gr.Markdown("å°šæœªå»ºç«‹")

    build_btn.click(build_index_from_folder, inputs=[ocr_lang, use_ocr, force_ocr], outputs=[build_status])
    peek_btn.click(list_catalog_pdfs, outputs=[build_status])
    key_btn.click(set_api_key_runtime, inputs=[key_in], outputs=[build_status])
    load_pkl_btn.click(_ui_load_cache_pkl, outputs=[build_status])
    save_pkl_btn.click(_ui_save_cache_pkl, outputs=[build_status])

    gr.Markdown("## â‘¡ æŸ¥è©¢ï¼ˆç‰‡æ®µå±¤ï¼‰")
    with gr.Tab("é—œéµå­—ç›´æ¥æœ"):
        with gr.Row():
            q1 = gr.Textbox(label="è¼¸å…¥éœ€æ±‚ï¼ˆä¾‹ï¼šã€å´ç‡ˆ 12W 3000K CRI90 24åº¦ã€ï¼‰", lines=2)
            topk1 = gr.Slider(1, 15, value=6, step=1, label="Top-K")
        btn1 = gr.Button("æœå°‹")
        out1 = gr.Markdown()
        btn1.click(ask_basic, inputs=[q1, topk1], outputs=[out1])

    with gr.Tab("AI ç†è§£å¾Œæœå°‹ï¼ˆLLM å…ˆè½‰æ›ï¼‰"):
        with gr.Row():
            q2 = gr.Textbox(label="ä¾‹å¦‚ï¼šã€é©åˆæµ´å®¤çš„å¸é ‚ç‡ˆã€ã€ã€å±•ç¤ºæ«ƒç”¨çª„è§’é«˜CRIã€", lines=2)
            topk2 = gr.Slider(1, 15, value=6, step=1, label="Top-K")
        btn2 = gr.Button("AI ç†è§£å¾Œæœå°‹")
        out2 = gr.Markdown()
        btn2.click(ask_ai, inputs=[q2, topk2], outputs=[out2])

    gr.Markdown("## â‘¢ ä»¥ç”¢å“ç‚ºä¸­å¿ƒï¼ˆå¢é›†å±¤ï¼‰")
    with gr.Tab("ç”¢å“é—œéµå­—æœå°‹ï¼ˆClusterï¼‰"):
        with gr.Row():
            q3 = gr.Textbox(label="è¼¸å…¥éœ€æ±‚ï¼ˆä¾‹ï¼šã€LED21st DL-123 IP65 å¸é ‚ç‡ˆã€ï¼‰", lines=2)
            topk3 = gr.Slider(1, 15, value=6, step=1, label="Top-K")
        btn3 = gr.Button("ä»¥ç”¢å“ç‚ºä¸­å¿ƒæœå°‹")
        out3 = gr.Markdown()
        btn3.click(ask_product, inputs=[q3, topk3], outputs=[out3])

    with gr.Tab("AI ç†è§£å¾Œç”¢å“æœå°‹ï¼ˆCluster + è¦æ ¼éæ¿¾ï¼‰ã€"):
        with gr.Row():
            q4 = gr.Textbox(label="ä¾‹å¦‚ï¼šã€é©åˆæµ´å®¤çš„ç‡ˆã€ã€ã€æˆ¶å¤–èµ°é“ç…§æ˜ã€æˆ–åŒ…å«å‹è™Ÿåç¨±", lines=2)
            topk4 = gr.Slider(1, 15, value=6, step=1, label="Top-K")
        btn4 = gr.Button("AI ç†è§£å¾Œç”¢å“æœå°‹")
        out4 = gr.Markdown()
        btn4.click(ask_product_ai, inputs=[q4, topk4], outputs=[out4])

# å•Ÿå‹•æ™‚ï¼ˆå¯é¸ï¼‰å˜—è©¦è¼‰å…¥ PKL
def try_load_pkl_on_start():
    folder = os.path.join(os.getcwd(), "catalogs")
    if not os.path.isdir(folder):
        return f"âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™å¤¾ï¼š{folder}"
    return load_cache_pkl(INDEX, folder)

if __name__ == "__main__":
    print(try_load_pkl_on_start())
    demo.launch()
