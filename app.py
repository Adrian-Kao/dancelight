# app.py â€” Lighting Catalog RAG (OCR + Folder Auto-Load + Cache)
# ä¾è³´ï¼špip install -U gradio pdfplumber sentence-transformers scikit-learn numpy pandas pillow pymupdf pytesseract
# å•Ÿå‹•å‰å…ˆé–‹è®Šæ•¸:venv\Scripts\activate
import os, io, re, pickle, logging, warnings
from typing import List, Dict, Any, Tuple

import numpy as np
import pandas as pd
from PIL import Image, ImageOps, ImageFilter

import gradio as gr
import pdfplumber
import fitz  # PyMuPDF
import pytesseract
from sentence_transformers import SentenceTransformer, CrossEncoder

# ---- éœéŸ³ä¸€äº›ä¸æ˜¯éŒ¯èª¤çš„å™ªéŸ³è¨Šæ¯ ----
logging.getLogger("pdfminer").setLevel(logging.ERROR)
warnings.filterwarnings("ignore", module="fitz")

# =========================
# åŸºæœ¬è¨­å®šï¼ˆå¯è‡ªè¡Œèª¿æ•´ï¼‰
# =========================
EMBED_MODEL_NAME = "BAAI/bge-m3"                 # å¤šèªå‘é‡æ¨¡å‹
RERANK_MODEL_NAME = "BAAI/bge-reranker-base"     # äº¤å‰ç·¨ç¢¼ rerank
USE_RERANK = True                                # åˆæœŸç‚º Trueï¼›è‹¥è¦æ›´å¿«å¯æ”¹ False

DEFAULT_OCR_LANG = "chi_tra+eng"                 # Tesseract èªè¨€
CHUNK_MAX_CHARS = 800
CHUNK_OVERLAP = 100
CACHE_PATH = "catalog_index.pkl"                 # å¿«å–æª”åï¼ˆæ”¾åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰

# =========================
# å˜—è©¦è‡ªå‹•è¨­å®š Tesseract è·¯å¾‘ï¼ˆWindows å¸¸è¦‹å®‰è£è·¯å¾‘ï¼‰
# =========================
def _maybe_set_tesseract_path():
    candidates = [
        r"C:\Program Files\Tesseract-OCR\tesseract.exe",
        r"C:\Tesseract-OCR\tesseract.exe",
    ]
    for path in candidates:
        if os.path.exists(path):
            pytesseract.pytesseract.tesseract_cmd = path
            tessdata_dir = os.path.join(os.path.dirname(path), "tessdata")
            if os.path.isdir(tessdata_dir):
                os.environ["TESSDATA_PREFIX"] = tessdata_dir
            break
_maybe_set_tesseract_path()

# =========================
# åµŒå…¥èˆ‡ rerank
# =========================
embed_model = SentenceTransformer(EMBED_MODEL_NAME)
reranker = CrossEncoder(RERANK_MODEL_NAME) if USE_RERANK else None

def embed_passages(texts: List[str]) -> np.ndarray:
    return embed_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True, batch_size=64)

def embed_query(q: str) -> np.ndarray:
    return embed_model.encode([q], convert_to_numpy=True, normalize_embeddings=True)[0]

# =========================
# æŸ¥è©¢åŒç¾©è©æ­£è¦åŒ–ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰
# =========================
SYNONYMS = {
    "æš–ç™½": "3000K", "é»ƒå…‰": "3000K",
    "è‡ªç„¶å…‰": "4000K", "ç™½å…‰": "5000K",
    "æµæ˜": "lm", "äº®åº¦": "lm", "åŠŸç‡": "W", "ç“¦æ•¸": "W",
    "é˜²æ°´": "IP", "é¡¯è‰²": "CRI", "é¡¯è‰²æŒ‡æ•¸": "CRI",
    "è§’åº¦": "beam", "å…‰æŸè§’": "beam",
    "è»Œé“ç‡ˆ": "track light", "æŠ•å…‰ç‡ˆ": "flood light",
    "å´ç‡ˆ": "downlight", "æŠ•å°„ç‡ˆ": "flood light"
}
def normalize_query(q: str) -> str:
    out = q
    for k, v in SYNONYMS.items():
        out = out.replace(k, v)
    return out

# =========================
# OCR èˆ‡æŠ½å–ï¼ˆå«å¼·åˆ¶ OCRã€æé«˜ DPI èˆ‡ Tesseract configï¼‰
# =========================
def _visible_char_count(s: str) -> int:
    return len(re.findall(r"[A-Za-z0-9\u4e00-\u9fff]", s))

def ocr_pdf_page(fitz_page, dpi: int = 400, lang: str = DEFAULT_OCR_LANG) -> str:
    mat = fitz.Matrix(dpi/72.0, dpi/72.0)
    pix = fitz_page.get_pixmap(matrix=mat, alpha=False)
    img = Image.open(io.BytesIO(pix.tobytes("png")))
    img = ImageOps.grayscale(img)
    img = ImageOps.autocontrast(img)
    img = img.filter(ImageFilter.MedianFilter(3))
    config = "--psm 6 --oem 3"  # é©åˆæ®µè½/è¡¨æ ¼å°å­—
    text = pytesseract.image_to_string(img, lang=lang, config=config)
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{2,}", "\n", text).strip()
    return text

def extract_pdf_chunks(pdf_path: str,
                       catalog_name: str,
                       use_ocr_fallback: bool = True,
                       ocr_lang: str = DEFAULT_OCR_LANG,
                       force_ocr: bool = False,
                       max_chars: int = CHUNK_MAX_CHARS,
                       overlap: int = CHUNK_OVERLAP) -> List[Dict[str, Any]]:
    chunks = []
    try:
        with pdfplumber.open(pdf_path) as pdf, fitz.open(pdf_path) as doc:
            for i, page in enumerate(pdf.pages, start=1):
                text = ""
                # éå¼·åˆ¶æ™‚å…ˆè©¦æ–‡å­—å±¤
                if not force_ocr:
                    try:
                        text = page.extract_text() or ""
                        text = re.sub(r"[ \t]+", " ", text)
                        text = re.sub(r"\n{2,}", "\n", text).strip()
                    except Exception:
                        text = ""

                need_ocr = force_ocr
                if not force_ocr:
                    if _visible_char_count(text) < 20 and use_ocr_fallback:
                        need_ocr = True

                if need_ocr:
                    try:
                        text = ocr_pdf_page(doc[i-1], dpi=400, lang=ocr_lang)
                    except Exception:
                        text = text  # å¯èƒ½ä»ç‚ºç©º

                if not text:
                    continue

                # åˆ‡å¡Š
                start = 0
                while start < len(text):
                    end = min(start + max_chars, len(text))
                    piece = text[start:end]
                    chunks.append({"catalog": catalog_name, "page": i, "text": piece})
                    if end == len(text):
                        break
                    start = max(0, end - overlap)
    except Exception:
        return []
    return chunks

# =========================
# In-memory ç´¢å¼•
# =========================
class InMemoryIndex:
    def __init__(self):
        self.docs: List[Dict[str, Any]] = []
        self.embs: np.ndarray | None = None
        self.built = False

    def reset(self):
        self.docs, self.embs, self.built = [], None, False

    def add_docs(self, docs: List[Dict[str, Any]]):
        self.docs.extend(docs)

    def build(self):
        if not self.docs:
            self.embs, self.built = None, False
            return "æ²’æœ‰å¯å»ºç«‹ç´¢å¼•çš„æ–‡ä»¶"
        texts = [d["text"] for d in self.docs]
        self.embs = embed_passages(texts)
        self.built = True
        return f"ç´¢å¼•å»ºç«‹å®Œæˆï¼Œå…± {len(self.docs)} å€‹ç‰‡æ®µ"

    def search(self, query: str, top_k: int = 8) -> List[Tuple[int, float]]:
        if not self.built or self.embs is None:
            return []
        q = normalize_query(query)
        q_emb = embed_query(q)
        sims = (self.embs @ q_emb)

        k = min(max(top_k * 5, top_k), len(sims))
        cand_idx = np.argpartition(-sims, range(k))[:k]
        pairs = [(int(i), float(sims[i])) for i in cand_idx]
        pairs.sort(key=lambda x: x[1], reverse=True)
        pairs = pairs[:max(top_k, 1)]

        if USE_RERANK and reranker is not None and len(pairs) > 0:
            q_dup = [q] * len(pairs)
            cand_texts = [self.docs[i]["text"] for i, _ in pairs]
            scores = reranker.predict(list(zip(q_dup, cand_texts)))
            reranked = list(zip([p[0] for p in pairs], scores))
            reranked.sort(key=lambda x: x[1], reverse=True)
            pairs = reranked[:top_k]
        return pairs

INDEX = InMemoryIndex()

# =========================
# å¿«å–ï¼šå„²å­˜ / è¼‰å…¥ / æ¸…é™¤
# =========================
def save_cache(index: InMemoryIndex) -> str:
    with open(CACHE_PATH, "wb") as f:
        pickle.dump(index, f)
    return f"ğŸ’¾ ç´¢å¼•å·²å„²å­˜è‡³ {os.path.abspath(CACHE_PATH)}"

def load_cache() -> str:
    global INDEX
    if not os.path.exists(CACHE_PATH):
        return "âš ï¸ æ‰¾ä¸åˆ°å¿«å–æª”ï¼Œè«‹å…ˆå»ºç«‹ç´¢å¼•ã€‚"
    with open(CACHE_PATH, "rb") as f:
        INDEX = pickle.load(f)
    return f"âœ… å·²è¼‰å…¥å¿«å–ï¼Œå¯ç›´æ¥æŸ¥è©¢ï¼ˆç‰‡æ®µï¼š{len(INDEX.docs)}ï¼‰"

def clear_cache() -> str:
    if os.path.exists(CACHE_PATH):
        os.remove(CACHE_PATH)
        return "ğŸ§¹ å·²åˆªé™¤å¿«å–æª” catalog_index.pkl"
    return "â„¹ï¸ æ²’æœ‰å¯åˆªé™¤çš„å¿«å–æª”"

# =========================
# å»ºç´¢å¼•ï¼ˆæ”¯æ´å¯¦æ™‚é€²åº¦å›å ±ï¼‰
# =========================
def list_catalog_pdfs() -> str:
    cwd = os.getcwd()
    folder = os.path.join(cwd, "catalogs")
    if not os.path.isdir(folder):
        return f"âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™å¤¾ï¼š{folder}"
    files = []
    for root, _, fs in os.walk(folder):
        for f in fs:
            if f.lower().endswith(".pdf"):
                files.append(os.path.join(root, f))
    if not files:
        return f"âš ï¸ {folder} å…§æ²’æœ‰ .pdf æª”æ¡ˆ"
    out = [f"ğŸ” ç›®å‰å·¥ä½œè·¯å¾‘ï¼š{cwd}", f"ğŸ“ƒ æ‰¾åˆ° {len(files)} æœ¬ PDFï¼š"]
    out += [" - " + p for p in files]
    return "\n".join(out)

def build_index_from_folder(ocr_lang: str, use_ocr: bool, force_ocr: bool, progress=gr.Progress(track_tqdm=True)):
    try:
        INDEX.reset()
        cwd = os.getcwd()
        folder = os.path.join(cwd, "catalogs")
        if not os.path.isdir(folder):
            yield f"âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™å¤¾ï¼š{folder}\nè«‹å»ºç«‹ catalogs/ ä¸¦æ”¾å…¥ PDFã€‚"
            return

        # æƒæ PDF
        pdf_files = []
        for root, _, files in os.walk(folder):
            for f in files:
                if f.lower().endswith(".pdf"):
                    pdf_files.append(os.path.join(root, f))
        if not pdf_files:
            yield f"âš ï¸ åœ¨ {folder} æ²’æ‰¾åˆ°ä»»ä½• PDFã€‚"
            return

        yield f"ğŸ” å·¥ä½œè·¯å¾‘ï¼š{cwd}\nğŸ“ å…± {len(pdf_files)} æœ¬ PDFï¼š\n" + "\n".join(" - "+p for p in pdf_files)

        total_chunks = 0
        for fi, pdf_path in enumerate(pdf_files, 1):
            base = os.path.basename(pdf_path)
            yield f"ğŸ“„ [{fi}/{len(pdf_files)}] é–‹å§‹è™•ç†ï¼š{base}"
            try:
                with fitz.open(pdf_path) as _doc:
                    if _doc.needs_pass:
                        yield f"âŒ æª”æ¡ˆåŠ å¯†éœ€å¯†ç¢¼ï¼š{pdf_path}"
                        continue
                    n_pages = len(_doc)

                progress(0, desc=f"OCR/æŠ½å– {base}...")
                chunks = extract_pdf_chunks(
                    pdf_path,
                    catalog_name=os.path.splitext(base)[0],
                    use_ocr_fallback=use_ocr,
                    ocr_lang=ocr_lang,
                    force_ocr=force_ocr
                )
                total_chunks += len(chunks)
                INDEX.add_docs(chunks)
                yield f"âœ… {base} åŠ å…¥ç‰‡æ®µï¼š{len(chunks)}ï¼ˆé æ•¸ï¼š{n_pages}ï¼‰"
            except Exception as e:
                yield f"âŒ è§£æå¤±æ•— {base} â†’ {type(e).__name__}: {e}"

        if not INDEX.docs:
            yield "âš ï¸ æ²’æœ‰æˆåŠŸåŠ å…¥ä»»ä½•ç‰‡æ®µï¼Œè«‹æª¢æŸ¥ OCR è¨­å®šæˆ–å‹¾ã€å¼·åˆ¶ OCRã€å†è©¦ã€‚"
            return

        build_msg = INDEX.build()
        yield f"ğŸ‰ {build_msg}ï¼ˆç¸½ç‰‡æ®µï¼š{total_chunks}ï¼‰"
        # æˆåŠŸå¾Œè‡ªå‹•å„²å­˜å¿«å–
        yield save_cache(INDEX)

    except Exception as e:
        yield f"ğŸ’¥ ç™¼ç”ŸéŒ¯èª¤ï¼š{type(e).__name__}: {e}"

# =========================
# æŸ¥è©¢
# =========================
def ask(query: str, top_k: int = 6) -> pd.DataFrame:
    if not INDEX.built:
        return pd.DataFrame([{"æç¤º": "è«‹å…ˆå»ºç«‹ç´¢å¼•ï¼Œæˆ–é»ã€ğŸ”„ é‡æ–°è¼‰å…¥å¿«å–ã€"}])
    hits = INDEX.search(query, top_k=top_k)
    rows = []
    for idx, score in hits:
        d = INDEX.docs[idx]
        txt = d["text"].replace("\n", " ")
        if len(txt) > 220:
            txt = txt[:220] + "..."
        rows.append({
            "å‹éŒ„": d["catalog"],
            "é ç¢¼": d["page"],
            "ç›¸ä¼¼åº¦/åˆ†æ•¸": round(float(score), 4),
            "ç‰‡æ®µæ‘˜è¦": txt
        })
    return pd.DataFrame(rows)

def sample_queries():
    return (
        "å´ç‡ˆ 12W 3000K CRI90 24åº¦",
        "æˆ¶å¤– IP65 æŠ•å…‰ç‡ˆ 5000K æ„Ÿæ‡‰",
        "å±•ç¤ºæ«ƒ downlight 15~24 åº¦ ä¸çœ©å…‰"
    )

# =========================
# Gradio ä»‹é¢
# =========================
with gr.Blocks(title="Lighting Catalog RAG â€“ OCR + è‡ªå‹•è¼‰å…¥ + å¿«å–") as demo:
    gr.Markdown(
        "# ğŸ’¡ Lighting Catalog RAG â€“ OCR + è‡ªå‹•è¼‰å…¥ + å¿«å–\n"
        "æŠŠ PDF æ”¾åœ¨å°ˆæ¡ˆæ ¹ç›®çš„ **catalogs/** â†’ æŒ‰ã€Œæƒæä¸¦å»ºç«‹ç´¢å¼•ã€â†’ æŸ¥è©¢ã€‚\n"
        "é¦–æ¬¡å®Œæˆå¾Œæœƒè‡ªå‹•å„²å­˜ç´¢å¼•åˆ° `catalog_index.pkl`ï¼›ä¸‹æ¬¡å•Ÿå‹•æœƒè‡ªå‹•è¼‰å…¥ã€‚"
    )

    with gr.Row():
        ocr_lang = gr.Dropdown(choices=["chi_tra+eng", "chi_sim+eng", "eng"], value=DEFAULT_OCR_LANG, label="OCR èªè¨€ï¼ˆTesseractï¼‰")
        use_ocr = gr.Checkbox(value=True, label="éœ€è¦æ™‚è‡ªå‹•ä½¿ç”¨ OCRï¼ˆè™•ç†åœ–ç‰‡æ–‡å­—ï¼‰")
        force_ocr = gr.Checkbox(value=False, label="å¼·åˆ¶ OCRï¼ˆæ¯é éƒ½ OCRï¼‰")

    with gr.Row():
        build_btn = gr.Button("â‘  æƒæ catalogs ä¸¦å»ºç«‹ç´¢å¼•", scale=3)
        peek_btn = gr.Button("ğŸ” æª¢è¦– catalogs å…§å®¹", scale=1)
        reload_btn = gr.Button("ğŸ”„ é‡æ–°è¼‰å…¥å¿«å–", scale=1)
        clear_btn = gr.Button("ğŸ§¹ æ¸…é™¤å¿«å–", scale=1)

    build_status = gr.Markdown("å°šæœªå»ºç«‹")
    build_btn.click(build_index_from_folder, inputs=[ocr_lang, use_ocr, force_ocr], outputs=[build_status])
    peek_btn.click(list_catalog_pdfs, outputs=[build_status])
    reload_btn.click(load_cache, outputs=[build_status])
    clear_btn.click(clear_cache, outputs=[build_status])

    gr.Markdown("## â‘¡ æŸ¥è©¢")
    with gr.Row():
        q = gr.Textbox(label="è¼¸å…¥éœ€æ±‚ï¼ˆä¾‹ï¼šã€å´ç‡ˆ 12W 3000K CRI90 24åº¦ã€ï¼‰", lines=2)
        topk = gr.Slider(1, 15, value=6, step=1, label="å›å‚³æ•¸é‡ Top-K")
    ask_btn = gr.Button("æœå°‹")
    results = gr.Dataframe(headers=["å‹éŒ„", "é ç¢¼", "ç›¸ä¼¼åº¦/åˆ†æ•¸", "ç‰‡æ®µæ‘˜è¦"], wrap=True)
    ask_btn.click(ask, inputs=[q, topk], outputs=[results])

    with gr.Accordion("ğŸ“ æŸ¥è©¢ç¯„ä¾‹", open=False):
        ex1, ex2, ex3 = sample_queries()
        gr.Examples(examples=[[ex1], [ex2], [ex3]], inputs=[q], label="é»ä¸€ä¸‹å¡«å…¥æŸ¥è©¢")

# =========================
# å…¥å£
# =========================
if __name__ == "__main__":
    # è‹¥ä½ å®‰è£åœ¨éé è¨­è·¯å¾‘ï¼Œå¯æ‰‹å‹•æŒ‡å®šï¼š
    # pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
    # os.environ["TESSDATA_PREFIX"] = r"C:\Program Files\Tesseract-OCR\tessdata"

    # å•Ÿå‹•æ™‚è‡ªå‹•è¼‰å…¥å¿«å–ï¼ˆè‹¥å­˜åœ¨ï¼‰
    if os.path.exists(CACHE_PATH):
        try:
            with open(CACHE_PATH, "rb") as f:
                INDEX = pickle.load(f)
            print(f"âœ… å·²è‡ªå‹•è¼‰å…¥å¿«å–ï¼ˆç‰‡æ®µï¼š{len(INDEX.docs)}ï¼‰ã€‚å¯ç›´æ¥æŸ¥è©¢ã€‚")
        except Exception as e:
            print(f"âš ï¸ å¿«å–è¼‰å…¥å¤±æ•—ï¼š{type(e).__name__}: {e}")

    demo.launch()
