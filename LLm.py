# app1.py â€” RAG æŸ¥è©¢ï¼ˆè®€å–ç¾æˆ PKLï¼‰+ LLM æŸ¥è©¢è¦åŠƒ + å¿«æ·éµæŸ¥è©¢
# pip install -U gradio sentence-transformers numpy python-dotenv openai regex

import os, time, json, pickle, regex as re
import numpy as np
import gradio as gr
from dotenv import load_dotenv, find_dotenv
from sentence_transformers import SentenceTransformer
import time 
import random

# -----------------------------
# 0) ENV & OpenAI åˆå§‹åŒ–ï¼ˆæ–°/èˆŠ SDK çš†å¯ï¼‰
# -----------------------------
DOTENV_PATH = find_dotenv(usecwd=True)
load_dotenv(DOTENV_PATH or "", override=True)
OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY") or "").strip()

OPENAI = None
OPENAI_STATUS = []


def img_to_png_bytes(page, max_w=1280) -> bytes:
    """
    è½‰å‡ºè¼ƒå°çš„ JPEGï¼Œæ¸›å°‘ä¸Šå‚³å¤§å°èˆ‡éŒ¯èª¤ç‡ã€‚
    """
    # å…ˆä»¥ 1.5x æ¸²æŸ“ï¼ˆç•«è³ªOKä¸”ä¸æœƒçˆ†ï¼‰
    pix = page.get_pixmap(matrix=fitz.Matrix(1.5, 1.5))
    img = Image.open(io.BytesIO(pix.tobytes("png"))).convert("RGB")

    # è‹¥å¤ªå¯¬å°±ç¸®åœ–
    if img.width > max_w:
        h = int(img.height * (max_w / img.width))
        img = img.resize((max_w, h), Image.LANCZOS)

    buf = io.BytesIO()
    img.save(buf, format="JPEG", quality=75, optimize=True)
    return buf.getvalue()

def _init_openai():
    global OPENAI, OPENAI_STATUS
    OPENAI_STATUS.clear()
    if not OPENAI_API_KEY:
        OPENAI = None
        OPENAI_STATUS.append("no_key")
        return
    try:
        from openai import OpenAI  # new sdk
        OPENAI = OpenAI(api_key=OPENAI_API_KEY)
        OPENAI_STATUS.append("new_sdk_ok")
        return
    except Exception as e:
        OPENAI_STATUS.append(f"new_sdk_fail:{type(e).__name__}")
    try:
        import openai  # legacy sdk
        openai.api_key = OPENAI_API_KEY
        OPENAI = openai
        OPENAI_STATUS.append("legacy_sdk_ok")
        return
    except Exception as e:
        OPENAI = None
        OPENAI_STATUS.append(f"legacy_sdk_fail:{type(e).__name__}")

def _chat_once(messages, model="gpt-4o-mini", max_tokens=400, temperature=0.2):
    if OPENAI is None:
        raise RuntimeError("openai_client_not_ready")
    # new sdk
    if hasattr(OPENAI, "chat") and hasattr(OPENAI.chat, "completions"):
        resp = OPENAI.chat.completions.create(
            model=model, messages=messages, max_tokens=max_tokens, temperature=temperature
        )
        return resp.choices[0].message.content.strip()
    # legacy sdk
    import types
    if isinstance(OPENAI, types.ModuleType) and hasattr(OPENAI, "ChatCompletion"):
        resp = OPENAI.ChatCompletion.create(
            model=model, messages=messages, max_tokens=max_tokens, temperature=temperature
        )
        return resp["choices"][0]["message"]["content"].strip()
    raise RuntimeError("unsupported_openai_client")

def set_api_key_runtime(key: str) -> str:
    global OPENAI_API_KEY
    k = (key or "").strip()
    if not k.startswith("sk-"):
        return "âŒ çœ‹èµ·ä¾†ä¸åƒ OpenAI Keyï¼ˆéœ€ä»¥ sk- é–‹é ­ï¼‰ã€‚"
    os.environ["OPENAI_API_KEY"] = k
    OPENAI_API_KEY = k
    _init_openai()
    return f"âœ… å·²å¥—ç”¨ã€‚ç‹€æ…‹ï¼š{', '.join(OPENAI_STATUS)}"

_init_openai()
print(f"[dotenv] path={DOTENV_PATH or '(none)'} key_loaded={bool(OPENAI_API_KEY)} status={','.join(OPENAI_STATUS)}")

# -----------------------------
# 1) åµŒå…¥æ¨¡å‹ & ç´¢å¼•é¡åˆ¥
# -----------------------------
EMBED_MODEL_NAME = "BAAI/bge-m3"
embed_model = SentenceTransformer(EMBED_MODEL_NAME)

def _doc_get(d, key, default=""):
    return d.get(key, default) if isinstance(d, dict) else default

def _doc_text(d):
    return d.get("text", "") if isinstance(d, dict) else str(d)

class InMemoryIndex:
    def __init__(self):
        self.docs = []
        self.embs = None
        self.built = False

    def search(self, query: str, top_k: int = 6):
        if not self.built or self.embs is None or len(self.docs) == 0:
            return []
        q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]
        sims = self.embs @ q_emb
        k = min(max(top_k, 1), len(sims))
        idx = np.argpartition(-sims, k - 1)[:k]
        pairs = [(int(i), float(sims[i])) for i in idx]
        pairs.sort(key=lambda x: x[1], reverse=True)
        return pairs

    def load_from_pkl(self, path: str) -> str:
        """
        é«˜å®¹éŒ¯ PKL è®€å–ï¼š
          - InMemoryIndex ç‰©ä»¶
          - dict{docs, embs}
          - [docs, embs] / åªæœ‰ [docs]
          - æå£/æ··é›œï¼šå›é€€æˆç´”æ–‡å­—è¡Œåˆ—è¡¨
        """
        import re as _re

        def _try_pickle_bytes(raw: bytes):
            try:
                return pickle.loads(raw)
            except Exception:
                # æ‰¾ pickle header (\x80\x04) å˜—è©¦åˆ‡ç‰‡ååºåˆ—åŒ–
                for m in _re.finditer(b"\x80\x04", raw):
                    try:
                        return pickle.loads(raw[m.start():])
                    except Exception:
                        continue
                return None

        try:
            with open(path, "rb") as f:
                raw = f.read()

            obj = _try_pickle_bytes(raw)
            if obj is None:
                # å˜—è©¦æ–‡å­—ä¾†å›è½‰ç¢¼
                try:
                    txt = raw.decode("utf-8", errors="ignore")
                    raw2 = txt.encode("latin1", errors="ignore")
                    obj = _try_pickle_bytes(raw2)
                except Exception:
                    obj = None

            if obj is None:
                # æœ€å¾Œå›é€€ï¼šæŒ‰è¡Œç•¶æ–‡å­—
                txt = raw.decode("utf-8", errors="ignore")
                lines = [line.strip() for line in txt.splitlines() if line.strip()]
                self.docs = [{"catalog": "æœªçŸ¥", "text": ln} for ln in lines]
                self.embs = None
                self.built = False
                return f"âš ï¸ ä»¥ç´”æ–‡å­—å›é€€è¼‰å…¥ï¼š{len(self.docs)} è¡Œï¼ˆç„¡ embeddingsï¼‰"

            docs, embs = None, None
            if hasattr(obj, "docs") and hasattr(obj, "embs"):
                docs, embs = list(obj.docs), obj.embs
            elif isinstance(obj, dict):
                docs, embs = obj.get("docs"), obj.get("embs")
            elif isinstance(obj, (list, tuple)):
                if len(obj) >= 2 and isinstance(obj[0], (list, tuple)):
                    docs, embs = list(obj[0]), obj[1]
                elif len(obj) >= 1:
                    docs, embs = list(obj), None

            if docs is None:
                return "âŒ è¼‰å…¥å¤±æ•—ï¼šæœªåµæ¸¬åˆ°å¯ç”¨çš„ docsã€‚"

            if embs is not None and not isinstance(embs, np.ndarray):
                try:
                    embs = np.array(embs, dtype=np.float32)
                except Exception:
                    pass

            self.docs, self.embs = docs, embs
            self.built = embs is not None and len(docs) == (embs.shape[0] if hasattr(embs, "shape") else len(embs))

            if self.built:
                dim = int(self.embs.shape[1]) if hasattr(self.embs, "shape") and self.embs.ndim == 2 else "?"
                return f"âœ… å·²è¼‰å…¥å¿«å–ï¼šç‰‡æ®µ {len(self.docs)}ï¼Œç¶­åº¦ {dim}"
            else:
                return f"âš ï¸ å·²è®€å– docs={len(self.docs)}ï¼Œä½†ç¼ºå°‘ embeddingsã€‚"

        except Exception as e:
            return f"âŒ è¼‰å…¥å¤±æ•—ï¼š{type(e).__name__}: {e}"

# **å…¨åŸŸç´¢å¼•å¯¦ä¾‹ â€” åœ¨ä»»ä½•å‡½å¼å‰å°±å»ºç«‹ï¼Œé¿å… NameError**
INDEX = InMemoryIndex()

# -----------------------------
# 2) åµŒå…¥è¼”åŠ©ï¼ˆå¯é¸ï¼šè‡ªå‹•è£œåµŒå…¥ï¼‰
# -----------------------------
def embed_in_batches(texts, batch_size=128, progress=None):
    embs = []
    total = len(texts); t0 = time.time()
    for i in range(0, total, batch_size):
        if progress: progress(min(i/total, 0.98), desc=f"Embedding {i}/{total}")
        batch = texts[i:i+batch_size]
        embs.append(
            embed_model.encode(batch, convert_to_numpy=True, normalize_embeddings=True, batch_size=batch_size)
        )
    if progress: progress(1.0, desc=f"Done {total}/{total} in {time.time()-t0:.1f}s")
    return np.vstack(embs) if embs else np.zeros((0, 1024), dtype=np.float32)

# -----------------------------
# 3) åŸºç¤é—œéµå­—æŸ¥è©¢
# -----------------------------
def keyword_search(q: str, top_k: int = 6) -> str:
    if not INDEX.built:
        return "âš ï¸ è«‹å…ˆè¼‰å…¥ `catalog_index.pkl`ï¼ˆæˆ–è£œåµŒå…¥ï¼‰ã€‚"
    if not q.strip():
        return "è«‹è¼¸å…¥é—œéµå­—ã€‚"
    hits = INDEX.search(q.strip(), top_k=top_k)
    if not hits:
        return "âŒ æ²’æœ‰æ‰¾åˆ°ç›¸ç¬¦å…§å®¹ã€‚"
    out = []
    for idx, _ in hits:
        d = INDEX.docs[idx]
        sheet = _doc_get(d, "catalog", "é é¢")
        txt = _doc_text(d).replace("\n", " ")
        out.append(f"### {sheet}\nâ€¢ {txt[:240]}{'â€¦' if len(txt)>240 else ''}")
    return "\n\n".join(out)

# -----------------------------
# 4) LLM è¦åŠƒ â†’ æª¢ç´¢
# -----------------------------
_RULE_HINTS = [
    (r"æµ´å®¤|è¡›æµ´|æ½®æ¿•|å»æ‰€",        ["IP65","IP54","é˜²æ°´","é˜²æ½®","é˜²æ¿•"]),
    (r"æˆ¶å¤–|åº­é™¢|é™½å°|é›¨",          ["IP65","IP66","é˜²æ°´","è€å€™"]),
    (r"å»šæˆ¿|æ²¹ç…™",                  ["æ˜“æ¸…æ½”","é˜²æ²¹æ±™","IP44"]),
    (r"èµ°é“|ç„é—œ|æ¨“æ¢¯",             ["æ„Ÿæ‡‰","å¾®æ³¢æ„Ÿæ‡‰","å»£è§’","é˜²çœ©"]),
    (r"å±•ç¤ºæ«ƒ|å±•æ«ƒ|é™³åˆ—",           ["çª„è§’","15Â°","24Â°","é«˜CRI","Raâ‰¥90"]),
    (r"é–±è®€|è¾¦å…¬|æ›¸æˆ¿",             ["4000K","5000K","é˜²çœ©","ç„¡é »é–ƒ"]),
    (r"æ”å½±|å•†å“æ‹æ”",              ["é«˜CRI","Raâ‰¥90","Raâ‰¥95","é«˜æµæ˜"]),
]

def _rule_plan(q: str) -> dict:
    must, nice = [], []
    for pat, kws in _RULE_HINTS:
        if re.search(pat, q):
            nice.extend(kws)
    must += re.findall(r"\b\d{2,4}K\b", q, re.I)
    must += re.findall(r"\b\d{1,3}\s?W\b", q, re.I)
    must += re.findall(r"\bIP\s?\d{2}\b", q, re.I)
    must += re.findall(r"CRI\s*(?:â‰¥|â‰§)?\s*\d{2}", q, re.I)
    must = list(dict.fromkeys([m.replace(" ","") for m in must]))
    nice = list(dict.fromkeys(nice))
    return {"must_terms": must, "nice_to_have": nice, "negations": []}

def llm_plan_query(user_text: str) -> dict:
    if OPENAI is None:
        return _rule_plan(user_text)
    sys = ("ä½ æ˜¯ç‡ˆå…·é¡§å•ã€‚è«‹æŠŠä½¿ç”¨è€…éœ€æ±‚è½‰æˆ JSONï¼Œå« must_terms/nice_to_have/negations é™£åˆ—ï¼›"
           "ç”¨å¸¸è¦‹è¦æ ¼ï¼š3000Kã€20Wã€CRIâ‰¥90ã€IP65ã€15Â°/24Â°ã€é˜²æ°´/é˜²æ½®/é˜²çœ©/æ„Ÿæ‡‰/é«˜CRIï¼›åªå› JSONã€‚")
    usr = f"ä½¿ç”¨è€…éœ€æ±‚ï¼š{user_text}\nåªå› JSONï¼Œä¾‹å¦‚ï¼š{{\"must_terms\":[\"IP65\"],\"nice_to_have\":[\"é«˜CRI\"],\"negations\":[]}}"
    try:
        txt = _chat_once([{"role":"system","content":sys},{"role":"user","content":usr}], max_tokens=200)
        data = json.loads(txt)
        if not isinstance(data, dict): raise ValueError
        for k in ("must_terms","nice_to_have","negations"):
            if k not in data or not isinstance(data[k], list): data[k] = []
        return data
    except Exception:
        return _rule_plan(user_text)

def build_search_string(plan: dict) -> str:
    must = plan.get("must_terms", [])
    nice = plan.get("nice_to_have", [])
    tokens = must + must + nice  # must åŠ æ¬Š
    return " ".join(tokens) if tokens else ""

def search_by_planner(user_text: str, top_k: int = 6) -> str:
    if not INDEX.built:
        return "âš ï¸ è«‹å…ˆè¼‰å…¥ `catalog_index.pkl`ï¼ˆæˆ–è£œåµŒå…¥ï¼‰ã€‚"
    plan = llm_plan_query(user_text)
    q = build_search_string(plan) or user_text
    hits = INDEX.search(q, top_k=top_k)
    if not hits:
        return f"âŒ æ‰¾ä¸åˆ°çµæœï¼ˆæŸ¥è©¢å­—ä¸²ï¼š{q}ï¼‰"

    spec_re = {
        "ç“¦æ•¸":   r"\b(\d{1,3}\s?W)\b",
        "è‰²æº«":   r"\b(\d{3,4}\s?K)\b",
        "CRI":    r"(CRI\s*(?:â‰¥|â‰§)?\s*\d{2})",
        "å…‰æŸè§’": r"(\d{1,3}\s?(?:Â°|åº¦))",
        "IP":     r"\bIP\s?\d{2}\b",
        "æµæ˜":   r"\b\d{3,5}\s?lm\b",
        "ç‰¹æ€§":   r"(é˜²æ°´|é˜²æ½®|é˜²æ¿•|é˜²çœ©|æ„Ÿæ‡‰|é«˜CRI|è€å€™|ç„¡é »é–ƒ)"
    }
    lines = [f"**æŸ¥è©¢ç†è§£** â†’ must={plan.get('must_terms',[])}, nice={plan.get('nice_to_have',[])}"]
    for idx, _ in hits:
        d = INDEX.docs[idx]
        sheet = _doc_get(d, "catalog", "é é¢")
        text  = _doc_text(d).replace("\n"," ")
        bullets=[]
        for k, rg in spec_re.items():
            ms = list(re.finditer(rg, text, re.I))
            if ms:
                vals = list(dict.fromkeys([m.group(1) if m.lastindex else m.group(0) for m in ms]))
                bullets.append(f"- **{k}**ï¼š{', '.join(vals[:4])}")
        if not bullets:
            bullets.append(f"- å…§å®¹ï¼š{text[:160]}{'â€¦' if len(text)>160 else ''}")
        lines.append(f"### {sheet}\n" + "\n".join(bullets))
    return "\n\n".join(lines)

# -----------------------------
# 5) å¿«æ·éµæŸ¥è©¢
# -----------------------------
_QUICK_FEATURE_MAP = {
    "äº®åº¦(lm)": ["lm","æµæ˜","é«˜æµæ˜"],
    "è‰²æº«(K)":  ["K","3000K","4000K","5000K","6500K","æš–ç™½","ç™½å…‰","è‡ªç„¶å…‰"],
    "é¡¯è‰²(CRI)": ["CRI","Raâ‰¥80","Raâ‰¥90","é«˜CRI","CRIâ‰¥90"],
    "å…‰æŸè§’(Â°)": ["Â°","åº¦","15Â°","24Â°","36Â°","45Â°"],
    "é˜²è­·(IP)": ["IP65","IP66","IP54","é˜²æ°´","é˜²æ½®","è€å€™"],
    "åŠŸç‡(W)":  ["W","ç“¦","12W","15W","20W","30W","50W"],
}

def quick_feature_search(name_or_model: str, feature: str, top_k: int = 6) -> str:
    if not INDEX.built:
        return "âš ï¸ è«‹å…ˆè¼‰å…¥ `catalog_index.pkl`ï¼ˆæˆ–è£œåµŒå…¥ï¼‰ã€‚"
    if not name_or_model.strip():
        return "è«‹å…ˆè¼¸å…¥å‹è™Ÿæˆ–å“åé—œéµå­—ã€‚"
    seeds = _QUICK_FEATURE_MAP.get(feature, [])
    q = " ".join([name_or_model.strip()] + seeds)
    hits = INDEX.search(q, top_k=top_k)
    if not hits:
        return f"æ‰¾ä¸åˆ°ï¼š{name_or_model} çš„ã€Œ{feature}ã€ç›¸é—œè³‡è¨Šã€‚"
    out=[]
    for idx,_ in hits:
        d=INDEX.docs[idx]
        sheet=_doc_get(d,"catalog","é é¢")
        txt=_doc_text(d).replace("\n"," ")
        for kw in seeds+[feature, name_or_model]:
            k=kw.strip()
            if not k: continue
            try: txt = re.sub(re.escape(k), f"**{k}**", txt, flags=re.I)
            except re.error: pass
        out.append(f"### {sheet}\nâ€¢ {txt[:240]}{'â€¦' if len(txt)>240 else ''}")
    return "\n\n".join(out)

# -----------------------------
# 6) è¼‰å…¥/è¨ºæ–·/è£œåµŒå…¥
# -----------------------------
CACHE_PATH = "catalog_index.pkl"

def load_cache_btn(auto_embed: bool=False, progress=gr.Progress()):
    """å˜—è©¦è¼‰å…¥ï¼›è‹¥ç¼º embs ä¸”å‹¾é¸ auto_embed â†’ ç¾å ´è£œåµŒå…¥ä¸¦å¯« merged æª”"""
    global INDEX
    if not os.path.exists(CACHE_PATH):
        return f"âš ï¸ æ‰¾ä¸åˆ° `{CACHE_PATH}`ï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®ã€‚"
    msg = INDEX.load_from_pkl(CACHE_PATH)
    if msg.startswith("âœ…"):
        return msg
    if ("ç¼ºå°‘ embeddings" in msg or "ç„¡ embeddings" in msg) and auto_embed:
        texts = [_doc_text(d) for d in INDEX.docs]
        if not any(texts):
            return "âŒ docs å…§æ²’æœ‰å¯åµŒå…¥çš„æ–‡å­—ã€‚"
        INDEX.embs = embed_in_batches(texts, batch_size=128, progress=progress)
        INDEX.built = True
        merged = "catalog_index_merged.pkl"
        with open(merged, "wb") as f:
            pickle.dump({"docs": INDEX.docs, "embs": INDEX.embs}, f)
        return f"âœ… å·²è‡ªå‹•è£œåµŒå…¥ï¼šç‰‡æ®µ {len(texts)}ï¼Œç¶­åº¦ {INDEX.embs.shape[1]}\nğŸ’¾ å·²å¯«å‡º `{merged}`ï¼ˆä¸‹æ¬¡å¯ç›´æ¥è¼‰å…¥ï¼‰"
    return msg

def peek_pkl(path=CACHE_PATH):
    if not os.path.exists(path):
        return f"âŒ æª”æ¡ˆä¸å­˜åœ¨ï¼š{os.path.abspath(path)}"
    try:
        with open(path, "rb") as f:
            obj = pickle.load(f)
        info = {"type": str(type(obj))}
        if hasattr(obj, "docs") and hasattr(obj, "embs"):
            info["summary"] = f"InMemoryIndex-like: docs={len(obj.docs)}, embs={getattr(obj.embs,'shape',None)}"
        elif isinstance(obj, dict):
            info["summary"] = f"dict keys={list(obj.keys())}"
        elif isinstance(obj, (list, tuple)):
            info["summary"] = f"sequence len={len(obj)}; elem0={type(obj[0]) if obj else None}"
        else:
            info["summary"] = "unknown layout"
        return "ğŸ” PKL æ¢æ¸¬ï¼š\n```\n" + json.dumps(info, ensure_ascii=False, indent=2) + "\n```"
    except Exception as e:
        return f"âŒ è§£æå¤±æ•—ï¼š{type(e).__name__}: {e}"

def diagnostics():
    lines = []
    lines.append(f"ğŸ“¦ æª”æ¡ˆï¼š{os.path.abspath(CACHE_PATH)} å­˜åœ¨={os.path.exists(CACHE_PATH)}")
    if INDEX and getattr(INDEX, 'built', False) and INDEX.embs is not None:
        lines.append(f"âœ… ç´¢å¼•å¯ç”¨ï¼šç‰‡æ•¸={len(INDEX.docs)}, ç¶­åº¦={INDEX.embs.shape[1]}")
    else:
        lines.append("âš ï¸ ç´¢å¼•å°šæœªå¯ç”¨ï¼ˆæœªè¼‰å…¥æˆ–ç¼º embeddingsï¼‰")
    lines.append(f"ğŸ§  OpenAI ç‹€æ…‹ï¼š{', '.join(OPENAI_STATUS) or 'unknown'}")
    lines.append(f"ğŸ”¡ åµŒå…¥æ¨¡å‹ï¼š{EMBED_MODEL_NAME}")
    return "\n".join(lines)

# -----------------------------
# 7) Gradio ä»‹é¢
# -----------------------------
with gr.Blocks(title="Lighting Catalog â€” RAGï¼ˆPKLï¼‰") as demo:
    gr.Markdown("## ğŸ’¡ Lighting Catalog â€” RAGï¼ˆè¼‰å…¥ PKLï¼‰\n"
                "- å…ˆè¼‰å…¥ `catalog_index.pkl`ï¼›è‹¥åªæœ‰æ–‡å­—ç„¡å‘é‡ï¼Œå¯å‹¾é¸ã€Œè‡ªå‹•è£œåµŒå…¥ã€ã€‚\n"
                "- æä¾›ï¼šåŸºç¤é—œéµå­—æœå°‹ / AI ç†è§£å¾Œæœå°‹ / å¿«æ·éµæŸ¥è©¢ã€‚\n")

    with gr.Row():
        auto_embed_chk = gr.Checkbox(label="è‹¥ PKL ç„¡å‘é‡ï¼Œè‡ªå‹•è£œåµŒå…¥ä¸¦è¼¸å‡º merged æª”", value=False)
        load_btn = gr.Button("ğŸ“¦ è¼‰å…¥ catalog_index.pkl", scale=2)
        peek_btn = gr.Button("ğŸ” æª”æ¡ˆçµæ§‹æ¢æ¸¬", scale=1)
        diag_btn = gr.Button("ğŸ§ª è¨ºæ–·", scale=1)
        key_box = gr.Textbox(label="ï¼ˆå¯é¸ï¼‰è‡¨æ™‚è¨­å®š OpenAI API Keyï¼šsk-...", type="password")
        key_btn = gr.Button("å¥—ç”¨Key", scale=1)

    status = gr.Markdown("å°šæœªè¼‰å…¥ç´¢å¼•")
    load_btn.click(fn=load_cache_btn, inputs=[auto_embed_chk], outputs=[status])
    peek_btn.click(fn=peek_pkl, outputs=[status])
    diag_btn.click(fn=diagnostics, outputs=[status])
    key_btn.click(fn=set_api_key_runtime, inputs=[key_box], outputs=[status])

    gr.Markdown("### ğŸ” åŸºç¤é—œéµå­—æœå°‹")
    with gr.Row():
        q_basic = gr.Textbox(label="é—œéµå­—/å‹è™Ÿï¼ˆå¦‚ï¼š30W 3000K CRI90 / MX9ï¼‰", lines=2)
        topk_basic = gr.Slider(1, 20, value=6, step=1, label="Top-K")
    btn_basic = gr.Button("æœå°‹")
    out_basic = gr.Markdown()
    btn_basic.click(fn=keyword_search, inputs=[q_basic, topk_basic], outputs=[out_basic])

    gr.Markdown("### ğŸ¤– AI ç†è§£å¾Œæœå°‹ï¼ˆLLM å…ˆæŠŠéœ€æ±‚è½‰æˆæª¢ç´¢æ¢ä»¶ï¼‰")
    with gr.Row():
        q_ai = gr.Textbox(label="ä¾‹å¦‚ï¼šã€é©åˆæµ´å®¤çš„å¸é ‚ç‡ˆã€ã€ã€å±•ç¤ºæ«ƒç”¨çª„è§’é«˜CRIã€", lines=2)
        topk_ai = gr.Slider(1, 20, value=6, step=1, label="Top-K")
    btn_ai = gr.Button("AI ç†è§£å¾Œæœå°‹")
    out_ai = gr.Markdown()
    btn_ai.click(fn=search_by_planner, inputs=[q_ai, topk_ai], outputs=[out_ai])

    gr.Markdown("### âš¡ å¿«æ·éµæŸ¥è©¢ï¼ˆå‹è™Ÿ/å“å â†’ ä¸€éµçœ‹ç‰¹å¾µï¼‰")
    with gr.Row():
        quick_name = gr.Textbox(label="å‹è™Ÿ/å“åï¼ˆå¦‚ï¼šMX9 / èˆå…‰è»Œé“ç‡ˆ / 30W ç¥ç›¾ï¼‰")
        quick_feat = gr.Radio(choices=list(_QUICK_FEATURE_MAP.keys()), value="é˜²è­·(IP)", label="ç‰¹å¾µ")
        quick_topk = gr.Slider(1, 20, value=6, step=1, label="Top-K")
    btn_quick = gr.Button("æŸ¥é€™å€‹ç‰¹å¾µ")
    out_quick = gr.Markdown()
    btn_quick.click(fn=quick_feature_search, inputs=[quick_name, quick_feat, quick_topk], outputs=[out_quick])

if __name__ == "__main__":
    demo.launch()
